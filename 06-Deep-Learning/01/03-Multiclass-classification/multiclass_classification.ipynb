{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning week - Day 1 - Exercise 3\n",
    "\n",
    "You just defined and run your first neural network for a two-class classification problem. What if there are more than two classes? This notebook is a multiclass classification task : based on the input data $x$, tells whether the sample belongs to the first, second, third, ... category.\n",
    "\n",
    "# Create the data\n",
    "\n",
    "\n",
    "The `make_blob` function [(see documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) enables to draw : \n",
    "- an arbitrary number of data sample, argument `n_samples`\n",
    "- an arbitrary number of features per data sample, argument `n_features`\n",
    "- an arbitrary number of categories, argument `centers`\n",
    "- a distance between the categories, argument `cluster_std`\n",
    "\n",
    "There is also the `random_state` argument that allows to draw the data deterministically, in order to reproduce the same data. Two persons that choose the same random_state will have the same data.\n",
    "\n",
    "❓ **Question** ❓ Based on the documentation, generate data with : \n",
    "- 1200 samples\n",
    "- 8 features per sample\n",
    "- 7 categories of data\n",
    "- 8 as the distance between the categories\n",
    "\n",
    "Select a `random_state` equal to 1.\n",
    "\n",
    "Print the shape and check that it corresponds to (1200, 8) for `X` and (1200) for `y`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Thanks to matplotlib, plot two (arbitrary) dimensions of the input data. Each dot should be colored by the category it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Repeat the operation on other dimensions, to visualy that the data are not easily separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for now, `y` is the list of integers, each correspoding to the category of the related input data.\n",
    "It looks like `[3, 2, 2, 3, 0, 5, 1, 1, 0, 5, ...]` (in this example, we have 6 categories, from 0 to 5).\n",
    "\n",
    "However, for categorical task in Keras, the output should have a number of columns equal to the number of different categories. Each row, corresponding to an input data, is a list of the probabilities that this input belongs to the corresponding category. AS here, the probabilities to belong to each category is equal to 1, it should look like\n",
    "\n",
    "```\n",
    "[\n",
    "[0, 0, 0, 1, 0, 0], \n",
    "[0, 0, 1, 0, 0, 0], \n",
    "[0, 0, 1, 0, 0, 0], \n",
    "[1, 0, 0, 0, 0, 0], \n",
    "[0, 0, 0, 0, 0, 1], \n",
    "[0, 1, 0, 0, 0, 0],\n",
    "[0, 1, 0, 0, 0, 0],\n",
    "[1, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 1],\n",
    "...\n",
    "]\n",
    "```\n",
    "\n",
    "Each column corresponds to a category. Each row corresponds to a target, the 1 being the category the input data belongs to.\n",
    "\n",
    "To transform `y` to categories, use `to_categorical` function from Keras . \n",
    "\n",
    "\n",
    "❓ **Question** ❓ First print `y`, then apply it and store it into `y_cat` and reprint `y_cat` to see the new structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Split the dataset $X$ and $y_cat$  into a train and test set (size: 70/30%)\n",
    "\n",
    "Remark : Please call the variables `X_train`, `y_train`, `X_test` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For technical reasons, the data should be rescaled, so that the data are _approximately_ all in [-1, 1].\n",
    "To do so, the `StandardScaler` function from Scikit-Learn [(see documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) allows to do that easily.\n",
    "\n",
    "[Advanced notion] The technical reason for this rescaling will be seen during the week ;)\n",
    "\n",
    "The function should be applied as \n",
    "```\n",
    "SScaler = StandardScaler()\n",
    "SScaler.fit(X)             ### Used to fit the coefficients of the standardisation\n",
    "X = Sscaler.transform(X)   ### Used to rescale X\n",
    "```\n",
    "\n",
    "❓ **Question** ❓ Given that you splited you dataset into `X_train` and `X_test`, how would you perform this task? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Complete the following function to initialize a model that has \n",
    "- a first layer with 50 neurons (activation being `relu` and appropriate input dimension)\n",
    "- a output layer designed for a multiclassification task which outputs probabilities for each classe\n",
    "\n",
    "Hint: the last layer will look like : `model.add(layers.Dense(SOME_NUMBER, activation='softmax'))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    \n",
    "    # Model architecture\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Model optimization : Optimizer, loss and metric \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model \n",
    "\n",
    "### Note here that the loss is different! This is because the task is not with two categories only, therefore\n",
    "### the solver is somehow different (will see it tomorrow)\n",
    "\n",
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ How many parameters (a.k.a. weights) are there in the model? How many a logistic regression would have had with the same data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Fit your model onto the train data with 50 epochs and plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Evaluate your model on the test set and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Is this a good score? You should compare it to some sort of benchmark value. In this case, what score would a random guess give?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ Wait ... If you get a closer look at the plot of the loss, it seems that the loss was still decreasing after 50 epochs. Why stopping it so soon? Let's rerun the model (with the initialization first) with 2000 epochs and plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ What can you say about the new loss? \n",
    "\n",
    "❓ **Question** ❓ Evaluate once again your model on the test set and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ On the one hand, the loss (computed on the train set) seems smaller than with 50 epochs. However, the accuracy on the test set got worse than before... \n",
    "\n",
    "❓ **Question** ❓ How is phenomenon called? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ The overfitting occurs at some point during the iterations, once the accuracy starts getting worse on the test set. Therefore, there is a need to stop the model at some point : we will see how to do that tomorrow.\n",
    "\n",
    "Nevertheless, we will see what happends in practice.\n",
    "\n",
    "❓ **Question** ❓ Run the following command and plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=500, \n",
    "                    batch_size=16,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Plot the values of the loss and accuracy on the train set (in blue) and on the test set (in orange). What can you comment on that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Reproduce similar results by defining a new architecture that includes : \n",
    "\n",
    "- a first layer with 25 neurons \n",
    "- a second layer with 15 neurons\n",
    "- a third layer with 10 neurons\n",
    "- a final layer that outputs probability for each class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_2():\n",
    "    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Fit this model with the previous data and look at the loss and accuracy through the iterations\n",
    "\n",
    "Hint : select a high enough epoch number to see the overfitting happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark 1** ❗ We clearly see that an overfitting can happend during the training. Tomorrow, we will see how to preven the algorithm from overfitting. \n",
    "\n",
    "❗ **Remark 2** ❗ The model overfits as the number of parameters is very very large (compare the number of weights with a logistic regression on the same data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
