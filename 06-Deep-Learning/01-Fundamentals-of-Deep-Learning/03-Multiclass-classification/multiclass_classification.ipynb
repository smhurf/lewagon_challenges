{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n",
    "We've just solved a binary classification problem. What about a multiclass one?\n",
    "\n",
    "### Exercise Objectives:\n",
    "- Write a Neural Network for multiclass classification\n",
    "- Observe overfitting during the model convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create the data\n",
    "\n",
    "\n",
    "The `make_blob` function [(see documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) enables to draw : \n",
    "- an arbitrary number of data sample, argument `n_samples`\n",
    "- an arbitrary number of features per data sample, argument `n_features`\n",
    "- an arbitrary number of categories, argument `centers`\n",
    "- a distance between the categories, argument `cluster_std`\n",
    "\n",
    "There is also the `random_state` argument that allows to draw the data deterministically, in order to reproduce the same data. Two persons that choose the same random_state will have the same data.\n",
    "\n",
    "‚ùì **Question** ‚ùì Based on the documentation, generate data with : \n",
    "- 1200 samples\n",
    "- 8 features per sample\n",
    "- 7 categories of data\n",
    "- 8 as the distance between the categories\n",
    "\n",
    "Select a `random_state` equal to 1.\n",
    "\n",
    "Print the shape and check that it corresponds to (1200, 8) for `X` and (1200) for `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Thanks to matplotlib, scatter plot two (arbitrary) dimensions of the input data together. Each dot should be colored by the category it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Repeat the operation on other dimensions, to visualy that the data are not easily separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for now, `y` is the list of integers, each correspoding to the category of the related input data.\n",
    "It looks like `[3, 2, 2, 3, 0, 5, 1, 1, 0, 5, ...]` (in this example, we have 7 categories, from 0 to 6).\n",
    "\n",
    "However, for categorical task in Keras, the **output should have a number of columns equal to the number of different categories**. Each row, corresponding to an input data, is a list of the probabilities that this input belongs to the corresponding category. As here, the probabilities to belong to each category is equal to 1, it should look like\n",
    "\n",
    "```\n",
    "[\n",
    "[0, 0, 0, 1, 0, 0, 0], \n",
    "[0, 0, 1, 0, 0, 0, 0], \n",
    "[0, 0, 1, 0, 0, 0, 0], \n",
    "[1, 0, 0, 0, 0, 0, 0], \n",
    "[0, 0, 0, 0, 0, 1, 0], \n",
    "[0, 1, 0, 0, 0, 0, 0],\n",
    "[0, 1, 0, 0, 0, 0, 0],\n",
    "[1, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 1],\n",
    "...\n",
    "]\n",
    "```\n",
    "\n",
    "Each column corresponds to a category. Each row corresponds to a target, the 1 being the category the input data belongs to.\n",
    "\n",
    "To transform `y` to categories, use `to_categorical` function from Keras . \n",
    "\n",
    "\n",
    "‚ùì **Question** ‚ùì First print `y`, then apply it and store it into `y_cat` and reprint `y_cat` to see the new structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Split the dataset $X$ and $y_{cat}$  into a train and test set (size: 70/30%)\n",
    "\n",
    "Remark : Please call the variables `X_train`, `y_train`, `X_test` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, the data should always be standard-scaled, so as to lay _approximately_ in [-1, 1]. (We will see later why).\n",
    "\n",
    "‚ùì **Question** ‚ùì Fit a sklearn [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) on the train set and transform both your train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Complete the following function to initialize a model that has \n",
    "- a first layer with 50 neurons (activation being `relu` and appropriate input dimension)\n",
    "- a output layer designed for a multiclassification task which outputs probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    \n",
    "    ### Model architecture\n",
    "    # To complete\n",
    "    # To complete\n",
    "    # To complete\n",
    "    \n",
    "    ### Model optimization : Optimizer, loss and metric \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model \n",
    "\n",
    "### Note here that the loss is different!\n",
    "### This is because the task is not with two categories only, therefore\n",
    "### the solver is somehow different (will see it tomorrow)\n",
    "\n",
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì How many parameters (a.k.a. weights) are there in the model? How many a logistic regression would have had with the same data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Fit your model onto the train data with 50 epochs and plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Evaluate your model on the test set and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Is this a good score? You should compare it to some sort of benchmark value. In this case, what score would a random guess give?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_baseline = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('baseline',\n",
    "                         accuracy=accuracy_baseline)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remark** ‚ùó Wait ... If you get a closer look at the plot of the loss, it seems that the loss was still decreasing after 50 epochs. Why stopping it so soon? Let's rerun the model (with the initialization first) with 1000 epochs and plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì \n",
    "- What can you say about the new loss? \n",
    "- Evaluate once again your model on the test set and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remark** ‚ùó On the one hand, the loss (computed on the train set) seems smaller than with 50 epochs. However, the accuracy on the test set got worse than before... \n",
    "\n",
    "‚ùì **Question** ‚ùì How is phenomenon called? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remark** ‚ùó The overfitting occurs at some point during the iteration of the gradient descent, once the accuracy starts getting worse on the test set. Therefore, there is a need to stop the fitting at some point.\n",
    "\n",
    "Let's see when does the test loss increases in practice. (Yes, we data-leak, we should create a validation set for that in reality...)\n",
    "\n",
    "‚ùì **Question** ‚ùì Run the following command and plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=500, \n",
    "                    batch_size=16,\n",
    "                    verbose=0)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Plot the values of the loss and accuracy on the train set (in blue) and on the test set (in orange). What can you comment on that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Reproduce similar results by defining a more complex architecture that includes : \n",
    "\n",
    "- a first layer with 25 neurons \n",
    "- a second layer with 15 neurons\n",
    "- a third layer with 10 neurons\n",
    "- a final layer that outputs probability for each class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_2():\n",
    "    # To complete entirely\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "### Answer ###\n",
    "##############\n",
    "\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remark** ‚ùó \n",
    "- We clearly see that an overfitting can happend during the training. More in our next lecture\n",
    "- The model overfits as the number of parameters is very very large (compare the number of weights with a logistic regression on the same data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üèÅ Congratulation! Commit and push your notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
