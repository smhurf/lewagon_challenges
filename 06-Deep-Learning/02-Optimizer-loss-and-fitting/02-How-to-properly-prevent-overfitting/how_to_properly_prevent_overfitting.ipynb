{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to properly prevent overfitting\n",
    "\n",
    "**Objective**\n",
    "- Give a validation set to the model\n",
    "- Use the stopping criterion to prevent the Neural network from overfitting\n",
    "- Regularize your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "First, let's generate some data thanks to the [`make_blob`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) function that we used yesterday.\n",
    "\n",
    "‚ùì **Question** ‚ùì Generate 2000 samples, with 10 features each. There should be 8 classes of blobs (`centers` argument), wich `cluster_std` equal to 7. Plot some dimensions to check your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T09:13:35.826987Z",
     "start_time": "2021-04-20T09:13:35.696628Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Use the `to_categorical` function from `tensorflow` to convert `y` to `y_cat` which is the categorical representation of `y` with one-hot encoding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part I : Proper cross-validation\n",
    "\n",
    "In a previous challenge, we split the dataset into a train and a test set at the beginning of the notebook. And then, we started to build different models which were trained on the train set but evaluated on the test set.\n",
    "\n",
    "So, at the end of the day, we used the test set as many times as we evaluated our models and different hyperparameters. We therefore _used_ the test set to select our best model, which is a sort of overfitting.\n",
    "\n",
    "A first good practice is to avoid using `random_state` or any deterministic separation between your train and test set. In that case, your test set will change everytime you re-run your notebook. But this is far from being sufficient.\n",
    "\n",
    "To properly compare models, you have to run a proper cross-validation, a 10-fold split for instance. Let's see how to do it properly.\n",
    "\n",
    "‚ùì **Question** ‚ùì First, write a function that outputs a neural network with 3 layers\n",
    "- a layer with 25 neurons, the `relu` activation function and the appropriate `input_dim`\n",
    "- a layer with 10 neurons and the `relu` activation function.\n",
    "- a last layer which is suited to the problem at hand (multiclass classification)\n",
    "\n",
    "The function should include its compilation, with the `categorical_crossentropy` loss, the `adam` optimizer and the `accuracy` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:19:05.297810Z",
     "start_time": "2021-04-19T15:19:05.295243Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:19:05.305507Z",
     "start_time": "2021-04-19T15:19:05.300766Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will do a proper cross validation.\n",
    "\n",
    "‚ùì **Question** ‚ùì Write a loop thanks to the [K-Fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) function of Scikit-Learn (select 10 splits) to fit your model on the train data, and evaluate it on the test data. Store the result of the evaluation in the `results` variable.\n",
    "\n",
    "Do not forget to standardize your train data before fitting the neural network.\n",
    "Also, 150 epochs shoul be sufficient in a first approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:19:05.342885Z",
     "start_time": "2021-04-19T15:19:05.308384Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "results = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "\n",
    "    # Split the data into train and test\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Initialize the model\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Fit the model on the train data\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Evaluate the model on the test data and append the result in the `results` variable\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Print the mean accuracy, and its standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:20:28.373148Z",
     "start_time": "2021-04-19T15:20:28.367805Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remark** ‚ùó You probably encountered one of the drawback of using a proper cross-validation for a neural network: **it takes a lot of time**. Therefore, for the rest of deep-learning module, we will do **only one split**. But remember that this is not entirely correct and, for real-life applications and problems, you are encouraged to use a proper cross-validation technique.\n",
    "\n",
    "‚ùó **Remark** ‚ùó In general, what practitioners do, is that they split only once, as you did. And once they get to the end of their optimization, they launch a real cross-validation at 6pm, go home and get the final results on the next day.\n",
    "\n",
    "‚ùì **Question** ‚ùì For the rest of the exercise (and of the deep-learning module), split the dataset into train and test with a 70/30% training to test data ratio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:20:28.379004Z",
     "start_time": "2021-04-19T15:20:28.375641Z"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II : Stop the learning before overfitting\n",
    "\n",
    "Let's first show that if we train the model for too long, it will overfit the training data and will not be good on the test data.\n",
    "\n",
    "‚ùì **Question** ‚ùì To do that, train the same neural network (do not forget to re-initialize it) with `validation_data=(X_test, y_test)` and 500 epochs. Store the history in the `history` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:20:28.390338Z",
     "start_time": "2021-04-19T15:20:28.388118Z"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Evaluate the model on the test set and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:21:10.154647Z",
     "start_time": "2021-04-19T15:21:10.152114Z"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Plot the history of the model with the following function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:29:59.503878Z",
     "start_time": "2021-04-19T15:29:59.498538Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history, title=None):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(13,5))\n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('Model loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylim((0,3))\n",
    "    ax[0].legend(['Train', 'Test'], loc='best')\n",
    "    \n",
    "    ax[1].plot(history.history['accuracy'])\n",
    "    ax[1].plot(history.history['val_accuracy'])\n",
    "    ax[1].set_title('Model Accuracy')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Test'], loc='best')\n",
    "    ax[1].set_ylim((0,1))\n",
    "    if title:\n",
    "        fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:21:10.221065Z",
     "start_time": "2021-04-19T15:21:10.219085Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see that the number of epochs we choose has a great influence on the final results: \n",
    "- If not enough epochs, then the algorithm is not optimal as it has not converged yet. \n",
    "- On the other hand, if too many epochs, we overfit the training data and the algorithm does not generalize well on test data.\n",
    "\n",
    "What we want is basically to stop the algorithm when the test loss is minimal (or the test accuracy is maximal).\n",
    "\n",
    "Let's introduce the early stopping criterion which is a way to stop the epochs of the algorithm at a interesting epoch. It basically use part of the data to see if the test loss stops from improving. You cannot use the test data to check that, otherwise, it is some sort of data leakage. On the contrary, it uses a subset of the initial training data, called the **validation set**\n",
    "\n",
    "It basically looks like the following : \n",
    "\n",
    "<img src=\"validation_set.png\" alt=\"Validation set\" style=\"height:350px;\"/>\n",
    "\n",
    "To split this data, we use, in the `fit` function, the `validation_split` keyword which sets the percentage of data from the initial training set used in the validation set. On top of that, we use the `callbacks` keyword to call the early stopping criterion at the end of each epoch. You can check additional information in the [documentation](https://www.tensorflow.org/guide/keras/train_and_evaluate)\n",
    "\n",
    "\n",
    "‚ùì **Question** ‚ùì Launch the following code, plot the history and evaluate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:21:12.643470Z",
     "start_time": "2021-04-19T15:21:10.548410Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping()\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "# Fit the model on the train data\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.3,\n",
    "                    epochs=500,\n",
    "                    batch_size=16, \n",
    "                    verbose=0, \n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:21:12.643470Z",
     "start_time": "2021-04-19T15:21:10.548410Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remark** ‚ùó The problem, with this type of approach, is that as soon as the loss of the validation set increases, the model stops. However, as neural network convergence is stochastic, it happens that the loss increases before decreasing again. For that reason, the Early Stopping criterion has the `patience` keyword that defines how many epochs without loss decrease you allow.\n",
    "\n",
    "‚ùì **Question** ‚ùì Use the early stopping criterion with a patience of 30 epochs, plot the results and print the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:21:12.648386Z",
     "start_time": "2021-04-19T15:21:12.645794Z"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remark** ‚ùó The model continues to converge even though it has some loss increase and descrease. The number of patience epochs to select is highly related to the task at hand and there does not exist any general rule. \n",
    "\n",
    "‚ùó **Remark** ‚ùó In case you select a high patience, you might face the problem that the loss on the test set decrease a lot from the best position. To that end, the early stopping criterion allows you to stop the convergence _and_ restore the weights of the neural network when it had the best score on the validation set, thanks to the `restore_best_weights` that is set to `False` by default.\n",
    "\n",
    "‚ùì **Question** ‚ùì Run the model with a early stopping criterion that enables to restore the best weights of the parameters, plot the loss and accuracy and print the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:21:17.270730Z",
     "start_time": "2021-04-19T15:21:17.268655Z"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remark 1** ‚ùó You can look at the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to play with other parameters\n",
    "\n",
    "‚ùó **Remark 2** ‚ùó No need to take a look at the epochs as long as it hit the stopping criterion. So, in the future, you should have a large number of epochs and the early stopping criterion has to stop the epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III : Batch-size & Epochs\n",
    "\n",
    "‚ùì **Question** ‚ùì Let's run the previous model with different batch sizes (with the early stopping criterion) and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:33:34.784883Z",
     "start_time": "2021-04-19T15:33:09.545758Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL (it can take some time)\n",
    "\n",
    "es = EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "for batch_size in [1, 4, 32]:\n",
    "    \n",
    "    model = initialize_model()\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_split=0.3,\n",
    "                        epochs=500,\n",
    "                        batch_size=batch_size, \n",
    "                        verbose=0, \n",
    "                        callbacks=[es])\n",
    "\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    plot_loss_accuracy(history, title=f'------ BATCH SIZE {batch_size} ------\\n The accuracy on the test set is of {results[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Look at the oscillations of the accuracy and loss with respect to the batch size number. Is this coherent with what we saw with the Tensorflow Playground? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:21:56.885224Z",
     "start_time": "2021-04-19T15:21:56.882115Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì How many optimizations of the weight are they within one epoch, with respect to the number of data and the batch size? Therefore, is one epoch longer with a large or a small bacth size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm455OX6ksyl"
   },
   "source": [
    "# Part IV: Regularization\n",
    "\n",
    "In this part of the notebook, we will see how to use regularizers in a neural network. Regularizers are used to prevent overfitting that can happends because very complex networks have many many parameters which tends to overfit the training data.\n",
    "\n",
    "First, let's initialize a model that has too many parameters for the task (many layers and/or many neurons) such that it overfits the training data  \n",
    "**To better see the effect, we will not use any early stopping criterion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T15:25:00.789411Z",
     "start_time": "2021-04-19T15:24:45.792165Z"
    },
    "executionInfo": {
     "elapsed": 81596,
     "status": "ok",
     "timestamp": 1612905145614,
     "user": {
      "displayName": "Bruno Lajoie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg0dl6gThG8gOPbCvHbgt62zQnsi8cgbQ7C5HkD_Cg=s64",
      "userId": "15793030209206844069"
     },
     "user_tz": -60
    },
    "id": "XaOTe0-Yksyn",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(25, activation='relu', input_dim=10))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "\n",
    "# Model compilation\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,  validation_split=0.3,\n",
    "                    epochs=300, batch_size=batch_size, verbose=0)\n",
    "\n",
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'The accuracy on the test set is of {results[1]:.2f}')\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03ZOjwm2ksyo"
   },
   "source": [
    "‚òùÔ∏è In our overparametrized network, some neurons got too specific to given training data, preventing the network from generalizing to new data. This lead to some overfitting. \n",
    "\n",
    "For that reason, we will use \n",
    "- [`Dropout`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layers, whose role is to _cancel_ the output of some neurons  during the training part. By doing this at random, it prevents the network from getting too specific to the input data : no any neuron can be too specific to a given input as its output is sometimes cancelled by the dropout layer. Overall, it forces the information that is contain in one input sample to go through multiple neurons instead of only one specific.\n",
    "\n",
    "- [`Regularizers`](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers), as in linear regression regularization where the weights of the linear regression are constrained by L1, L2 or L1 and L2 norms.\n",
    "\n",
    "‚ùì **Question** ‚ùì Try adding dropout layers and regularization to all your layers of the above neural network and look at the effect on the loss on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulation** \n",
    "\n",
    "Don't forget to commit and push your challenge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
