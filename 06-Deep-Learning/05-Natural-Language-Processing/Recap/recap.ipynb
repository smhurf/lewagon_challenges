{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "## Data \n",
    "\n",
    "Given the next function, load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "def load_data():\n",
    "    (sentences_train, y_train), (sentences_test, y_test) = imdb.load_data()\n",
    "    word_to_id = imdb.get_word_index()\n",
    "\n",
    "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "    for i, w in enumerate(['<PAD>', '<START>', '<UNK>', '<UNUSED>']):\n",
    "        word_to_id[w] = i\n",
    "\n",
    "    id_to_word = {v:k for k, v in word_to_id.items()}\n",
    "\n",
    "    sentences_train = [[id_to_word[_] for _ in sentence] for sentence in sentences_train]\n",
    "    sentences_test = [[id_to_word[_] for _ in sentence] for sentence in sentences_test]\n",
    "    \n",
    "    return sentences_train, y_train, sentences_test, y_test\n",
    "\n",
    "sentences_train, y_train, sentences_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using text, you have to convert your text into meaningful representation. You have two options to do that:\n",
    "- option #1: learn a representation, thanks to word2vec for instance, and then input it in the model\n",
    "- option #2: learn the representation directly in the model\n",
    "\n",
    "## Option #1: Word2Vec\n",
    "\n",
    "❓ Learn a word2vec representation, for which you can defined size of the embedding, the learning window and the minimum occurences of a word to count it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Convert your sentences to their embedded representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Pad your input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Initialize a model and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2: Embedding layer\n",
    "\n",
    "Instead of learning an embedding that is not related to the task you are working on, you can learn the vector representation of the words with an embedding layer in the model. \n",
    "\n",
    "❓ But first, we have to convert the words in the sentences to tokens (because word2vec does it internally). Create a dictionary whose keys are the words in the train set, and the values are unique integer identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Convert your train and test sentences to lists of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Pad your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Write, in a function, a model which has:\n",
    "- a masking layer not to take into account padded values\n",
    "- an embedding for which you can choose the embedding size\n",
    "- a LSTM or a GRU layer\n",
    "- a dense layer\n",
    "- an output layer\n",
    "\n",
    "Then, compile your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Fit your model on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}