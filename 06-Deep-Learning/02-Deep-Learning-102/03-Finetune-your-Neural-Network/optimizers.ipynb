{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Day 2 - Exercise 3\n",
    "\n",
    "Now that you have mastered almost every part of Neural Networks, let's take a closer look at the `compile` part.\n",
    "\n",
    "### Data\n",
    "\n",
    "We will here use the data from the Boston Housing dataset\n",
    "\n",
    "❓ **Question** ❓ First, load the data with the appropriate function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ To get a sense of a benchmark score you have to beat, what is the mean absolute error on the test set if your prediction corresponds to the mean value of $y$ computed on the train set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: The model\n",
    "\n",
    "❓ **Question** ❓ Now, write in a function `initialize_model` a neural network that has 3 layers: \n",
    "- a layer with 10 neurons and the `relu` activation function (appropriate input dimension)\n",
    "- a layer with 7 neurons and the `relu` activation function\n",
    "- an appropriate layer corresponding to the problem at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : The optimizer\n",
    "\n",
    "❓ **Question** ❓ Write a function that takes as argument a model and a name of optimizer, that compiles the model and returns it - select the loss and metrics wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, optimizer_name):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Initialize the model, compile it with the `adam` optimizer and fit it on the data. Evaluate your model on the test data.\n",
    "\n",
    "Do not forget to use an early stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Rerun the same model on the same data but with different optimizer (in a `for` loop). For each, plot the history and report the corresponding Mean Absolute Error. (see [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_mae(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['mae'])\n",
    "    plt.plot(history.history['val_mae'])\n",
    "    plt.title('Model Mean Absolute Error')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Are your predictions better than the benchmark prediction you evaluate at the beginning of the notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ Here, the optimizer is not central as the data are in low dimension and not many samples. However, in practice, you are advised to start with the `adam` optimizer by default which often works best. \n",
    "\n",
    "\n",
    "❗ **Remark** ❗ Internally, when you call any optimizer with a string, the neural network initializes the hyperparameters the optimizer rely on. Among this hyperparameters, there is quite an important one, the learning rate. This learning rate corresponds to the intensity of change of the weights at each optimization of the neural network. Different learning rates have different consequences, as shown here : \n",
    "\n",
    "<img src=\"learning_rate.png\" alt=\"Learning rate\" style=\"height:350px;\"/>\n",
    "\n",
    "\n",
    "As the learning rate is initialized with default values when you compile the model optimizer with a string, let's see how to do it differently.\n",
    "\n",
    "\n",
    "❓ **Question** ❓ Instead of initializing the optimizer with a string, we will initialize a real optimizer directly. Look at the documentation of [adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) and instanciate it with a learning rate of 0.1 - keep the other values to their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Use this optimizer in the `compile_model` function, fit the data and plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, reproduce the same plots and results but for different learning rates.\n",
    "\n",
    "Remark: There is a chance that the y-axis is too large for you to visualize the results. In that case, rewrite the plot function to plot only the epochs > 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_mae(history, n_epochs):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 5]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced & Optional\n",
    "\n",
    "The next question is optional and intended for advanced users that are willing to go a step further in their Deep Learning skills. However, it is not essential and can be skiped as many algorithms can be run without such optimization. \n",
    "\n",
    "Instead of keeping a fixed learning rate, you can change it from one iteration to the other, with the intuition that you first need large learning rates, and as the neural network converges and get closer to the minimum, you decrease the value of the learning rate. This is called a scheduler. \n",
    "\n",
    "❓ **Question** ❓ Use the [exponential decay scheduler](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) in the adam optimizer and run it on the previous data. Plot the history and check how the loss and mae evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : The loss\n",
    "\n",
    "It is important to clearly understand the different between metrics and loss. The loss are part of the metrics, therefore some metrics can be used as loss.\n",
    "\n",
    "❓ **Question** ❓ Run the same neural network, once with the `mae` as the loss, and once with the `mse`. In both case, report the final mean absolute error and compare the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you see that even if the Mean Absolute Error is the final metric you optimize, it can be better for your algorithm to use the mean square error as the loss to optimize its weights and give the best MAE possible. \n",
    "\n",
    "# Part IV : Save and load a model\n",
    "\n",
    "Here, we will see how to save a model, and load it afterwards.\n",
    "\n",
    "❓ **Question** ❓ Rerun any model you want and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, save the model thanks to the `save_model` method that you can find [here](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, in a variable `loaded_model`, load the model you just saved thanks to the `load_model` [(documentation here)](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model), and evaluate it on the test data to see that it gives the same result as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V : Use regularizers [ Advanced ]\n",
    "\n",
    "In this part of the notebook, we will see how to use regularizers in a neural network. Regularizers are used to prevent overfitting that can happends because very complex networks have many many parameters which tends to overfit the training data.\n",
    "\n",
    "❓ **Question** ❓ First, initialize a model that has a lot of parameters (many layers and/or many neurons) such that it overfits the training data (to better see the effect, do not use any early stopping criterion). \n",
    "\n",
    "NB: You can use the `validation_data` (instead of `validation_split`) to compute the error on the test set at each epoch.\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>(It might be not easy to overfit as the data as quite small - in term of number of features. Click here to get some hints)</summary>\n",
    "If you choose a neural networkk with the following layers:\n",
    "    \n",
    "    - 1000 neurons\n",
    "    \n",
    "    - 1000 neurons\n",
    "    \n",
    "    - 750 neurons\n",
    "    \n",
    "    - 750 neurons\n",
    "    \n",
    "    - 500 neurons\n",
    "    \n",
    "    - 500 neurons\n",
    "    \n",
    "And more than 2000 epochs (it will take some time to train),\n",
    "you should see some overfitting of the training data, meaning that the accuracy on the test lost will get worse at some point.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your overparametrized network, some neurons got too specific to given training data, preventing the network from generalizing to new data. This lead to some overfitting. \n",
    "\n",
    "For that reason, we will use dropout layers, whose role is to _cancel_ the output of some neurons  during the training part. By doing this at random, it prevents the network from getting too specific to the input data : no any neuron can be too specific to a given input as its output is sometimes cancelled by the dropout layer. Overall, it forces the information that is contain in one input sample to go through multiple neurons instead of only one specific.\n",
    "\n",
    "❓ **Question** ❓ Try adding dropout layers to your neural network and look at the effect on the loss on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways to prevent overfitting in a neural network. The standard one (apart Dropout) is to regularize the outputs of a layer, as in linear regression regularization where the weights of the linear regression are constrained by L1, L2 or L1 and L2 norms.\n",
    "\n",
    "Such regularization can be done at each layer for:\n",
    "- all the biases of the layer, with `bias_regularizer` argument\n",
    "- all the weights of the layer, with the `kernel_regularizer` argument\n",
    "- the outputs of the layer (after the activity function) with the `activity_regularizer` argument\n",
    "\n",
    "❓ **Question** ❓ Add some of the previous regularizers to the Dense layers of your previous neural networks - and without dropout layers, as it can interact in a way that you are not sure of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: You probably have seen very little difference in the previous examples. This is due to the simplicity of the data. However, in other cases, especially in the last exercice (open challenge), the data are more complex, and for some architectures, you might encounter overfitting that you can prevent thanks to regularization techniques as overfitting or L1/L2 regularizations.\n",
    "\n",
    "# Part V : Get the best Mean Absolute Error\n",
    "\n",
    "From here, you are free to:\n",
    "- design your own architecture: number of layers, number of neurons, activation function\n",
    "- choose the loss and metric\n",
    "- choose the optimizer and tune its hyperparameters\n",
    "- select the best options for the early stopping criterion\n",
    "\n",
    "to get the best mean absolute error.\n",
    "\n",
    "Once you have your best score on the test set, include the whole pipeline into a proper cross validation to report your score (mean ± std).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
