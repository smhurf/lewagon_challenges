{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression\n",
    "\n",
    "Thanks to the bagging algorithm, we get \"smoother\" boundaries. We are actually reducing the variance. Now, we will use the Random Forest Model (aka Features Bagging).\n",
    "\n",
    "With bagging, we randomly choose which **samples** to train on. With RF, we randomly choose which **features** to train on. but the principle stay the same ! We can choose the number of features to optimize our model but generally we take :\n",
    "```python\n",
    "import math\n",
    "d = math.floor(D/3) # for regression\n",
    "d = math.floor(D**0.5) # for classification\n",
    "```\n",
    "Before going further, you should have a look at the docs and make sure you understand how it works and all the options (max_depth, max_features, criterion,...):\n",
    "- [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "\n",
    "Some questions you could ask yourself :\n",
    "- What if most of the features are irrelevant ?\n",
    "- What if your RF model choose randomly only irrelevant features ?\n",
    "\n",
    "Generally, RF can deal with this. However, in some cases it can be problematic. Later, we'll learn about an algorithm called boosting that fixes this problem.\n",
    "\n",
    "In this exercise, we will work with more than just one feature because we saw in the previous exercise the \"smoothing effect of ensembling\" and RF makes no sense with just one feature.\n",
    "\n",
    "For this exercise, we will work **on a housing price dataset**. Our goal is to predict the price of a house given some attributes about it.\n",
    "\n",
    "Through this exercise, we are going to :\n",
    "- Standardize the data\n",
    "- Create a Linear Regression model;\n",
    "- Create a DecisionTree model;\n",
    "- Create a RF model;\n",
    "- Use Cross-validation and test score to compare each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "Before trying to solve this problem, have a look at the data : \n",
    "- check the head\n",
    "- check the type of each column\n",
    "- Try to understand the meaning of each column\n",
    "- what's the shape of the dataset\n",
    "- plot some data\n",
    "- ...\n",
    "\n",
    "As you will notice, the columns have no name. You could have a look at the dataset and the docs on [this website](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/) if you want more information. To help you, we create a variable `columns_name` in the following cell. This variable contains the names in the same orders as they appear in the dataset.\n",
    "\n",
    "Start by loading the data located in `data/housing.data` (you can use pandas `read_csv` method) and by renaming the columns with their corresponding names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "np.random.seed(10)\n",
    "\n",
    "columns_name = [\n",
    "    'crim', # numerical\n",
    "    'zn', # numerical\n",
    "    'nonretail', # numerical\n",
    "    'river', # BINARY\n",
    "    'nox', # numerical\n",
    "    'rooms', # numerical\n",
    "    'age', # numerical\n",
    "    'dis', # numerical\n",
    "    'rad', # numerical\n",
    "    'tax', # numerical\n",
    "    'ptratio', # numerical\n",
    "    'b', # numerical\n",
    "    'lstat', # numerical\n",
    "    'medv', # numerical -- this is the TARGET\n",
    "]\n",
    "\n",
    "# TO DO: add names to each column of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and standardize your data\n",
    "\n",
    "In this exercise, we will start by standardize our data and split them into 2 subsets : \n",
    "- **training data** (X_train, y_train) : 70% of the dataset\n",
    "- **test data** (X_test, y_test) : the rest of the dataset (30%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, standardize `X_train` using `StandardScaler` from Sklearn. You should fit the scaler on `X_train` and use it to transform `X_train` and `X_test`. Be careful, some column(s) should not be standardized (for example binary column(s) (0 or 1) and the target column `medv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your standardization by calculating the mean and std for each feature of `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `y_train` and `y_test`, replace the column by the log of each value. Why do we do that ? Imagine, you sell a house 10.000Sk and you realized you have made an error equivalent to 5.000Sk. It's a big mistake, right? Now, imagine,  you sell a house 1.000.000Sk and make an error of 5.000Sk. In comparison to the selling price, it's not that bad. Taking the log of the target column is a way to represent that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "The data are splitted and standardized, ... We can start to use our models ! We will compare the performance of 3 models : \n",
    "- Random Forest Regressor \n",
    "- Linear Regression\n",
    "- Decision Tree Regressor\n",
    "\n",
    "For each model, you have to:\n",
    "1. Initialize your model\n",
    "2. Train your model on the training set\n",
    "3. Make predictions on the test set\n",
    "4. **graph 1** : scatter plot the Ytest on the X_axis and the predictions on the Y_axis\n",
    "5. **graph 2** : plot the predictions and the Ytest\n",
    "6. Compute the test score and the cross validation score\n",
    "\n",
    "**Note** : the code for each model will be very similar... Do not hesitate to copy paste !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# TO DO:\n",
    "# 1. Initialize the RandomForestRegressor with n_estimators=50\n",
    "# 2. Train your model on the training set\n",
    "# 3. Make predictions on the test set\n",
    "\n",
    "\n",
    "# predictions[:5] should return : \n",
    "# array([3.24018446, 3.44032857, 3.23737663, 3.35909237, 3.03636152])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# Scatter plot Ytest on the X_axis and the predictions on the Y_axis\n",
    "# plot a line on the same graph with slope 0 and intercept 0 --> Can you guess why we do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# TO DO:\n",
    "# Print the cross_val_score and the score of the RF model\n",
    "\n",
    "\n",
    "\n",
    "# You should get : \n",
    "# -> CV forest: 0.7564990017784684\n",
    "# -> test score forest: 0.8555640751859941"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    "Well done ! We will do exactly the same for the Linear Regressor. Quick reminder : \n",
    "1. Initialize your model\n",
    "2. Train your model on the training set\n",
    "3. Make predictions on the test set\n",
    "4. **graph 1** : scatter plot the Ytest on the X_axis and the predictions on the Y_axis\n",
    "5. **graph 2** : plot the predictions and the Ytest\n",
    "6. Compute the test score and the cross validation score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# TO DO:\n",
    "# 1. Initialize the LinearRegression\n",
    "# 2. Train your model on the training set\n",
    "# 3. Make predictions on the test set\n",
    "\n",
    "\n",
    "# predictions[:5] should return : \n",
    "# [3.40505004 3.44102604 3.41337444 3.07337911 2.93213621]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# TO DO:\n",
    "# Scatter plot Ytest on the X_axis and the predictions on the Y_axis\n",
    "# plot a line on the same graph with slope 0 and intercept 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# TO DO:\n",
    "# Print the cross_val_score and the score of the Linear Regression\n",
    "\n",
    "\n",
    "# You should get : \n",
    "# -> CV baseline: 0.7299181763124764\n",
    "# -> test score baseline: 0.7754525837588145"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "Again, repeat the previous step for the DecisionTreeRegressor (train, predict, plot, measure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# TO DO:\n",
    "# 1. Initialize the LinearRegression\n",
    "# 2. Train your model on the training set\n",
    "# 3. Make predictions on the test set\n",
    "\n",
    "\n",
    "# predictions[:5] should return : \n",
    "# array([3.10009229 3.21084365 3.21084365 3.44041809 2.67414865])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# TO DO:\n",
    "# Scatter plot Ytest on the X_axis and the predictions on the Y_axis\n",
    "# plot a line on the same graph with slope 0 and intercept 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# TO DO:\n",
    "# Print the cross_val_score and the score of the Linear Regression\n",
    "\n",
    "\n",
    "# You should get : \n",
    "# -> CV baseline: 0.6449516714366336\n",
    "# -> test score baseline: 0.6228843203786647"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations ðŸŽ‰ðŸŽ‰ðŸŽ‰! Regarding to the graphs, the cross validation score and the test score of each model, which one is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
