{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first exercise, you will __Apply K-means__ in a 2D dataset to help gain an intuition of how the algorithm works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate & Plot Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`sklearn.datasets.make_blobs`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) is an `sklearn` method that uses a Gaussian equation to generate separable datasets for clustering or classification.\n",
    "\n",
    "üëâUse it to generate a test dataset with 2 features (X, y) using the following parameters:\n",
    "- 500 data points\n",
    "- 4 classes (or clusters)\n",
    "- `random_state`=42\n",
    "\n",
    "Plot what your data looks like (it should be 4 distinct clusters of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, \n",
    "\n",
    "1. You've created a fake dataset X, with no target at all (no supervision with a `y` vector).\n",
    "\n",
    "2. This dataset has 500 observations, and 2 dimensions (or features), which are represented as two-dimensional positions on the plot you just made.\n",
    "\n",
    "3. This dataset still has a structure, which is 4 clusters in our case with different centers (or `centroids`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "import numpy as np\n",
    "\n",
    "result = ChallengeResult('blobs', \n",
    "                         shape=X.shape, \n",
    "                         lower_centroid=X[np.logical_and(X[:,0] < -2.5, X[:,1] < -3)].mean(axis=0))\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to find automatically the clusters that gives you the inherent structure of your data. Those clusters are only represented by their center.\n",
    "\n",
    "What you have to determine though, is the __number of clusters__ that you think the data has.\n",
    "\n",
    "[`sklearn` has a `KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) package that does the math for you. \n",
    "\n",
    "üëâImport `KMeans` from `sklearn` and initiate a model with the follwing parameters:\n",
    "- `n_clusters=2`,\n",
    "- `random_state=42`\n",
    "\n",
    "üëâThen, fit the created data to the model, using `fit_predict`, which both fits the Kmeans model AND predicts back on the observations. \n",
    "**It hence returns an array of cluster index assignment for every observation in X**. Store it into the `y_pred` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now plot the prediction of clusters that KMeans gives you. In order to have a better overview of what the KMeans has found as clusters (here only 2), we want to color every observation with a distinct color corresponing to a different cluster. \n",
    "\n",
    "üëâIn order to do that, just plot your scattered observations, and pass the prediction vector that you just got as the color of the observation using `c=prediction_vector` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you must see, the KMeans has found 2 clusters as required. It's not the modeling we want. We want to find the optimal number of clusters that represents our data the best. How do we find that optimal number of clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('two_means', clusters=y_pred)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Appropriate Number of Clusters\n",
    "\n",
    "We can use two techniques to find the appropriate number of clusters. \n",
    "\n",
    "You have to know that the Kmeans also returns a property named `inertia_` that **returns the sum of squared distances of samples to their closest cluster center**. \n",
    "\n",
    "It can serve as an indicator to determine how much we have of **variance explained** the data by the found clusters. \n",
    "\n",
    "### The Elbow Method\n",
    "\n",
    "This is a technique that is used to help us find the appropriate number of cluster in K-Means. \n",
    "\n",
    "This method looks at the percentage of variance explained as a function of the number of clusters: one should choose a number of clusters so that __adding another cluster doesn't give much better modeling of the data__ (the marginal gain drops, giving an angle in the graph). Hence the \"elbow criterion\". \n",
    "\n",
    "NB: This \"elbow\" cannot always be unambiguously identified.\n",
    "\n",
    "üëâ__To do that, you want to `fit` a KMeans for every number of cluster between 1 and 10, and save the explained variance__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you want to plot the inertias as a function of the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an \"elbow\" where the inertia stops dropping dramatically at 4 (since we generated that data, we know it's the right answer üòâ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "\n",
    "From the previous Elbow Method we can say that the optimal clusters are 4. This can also be confirmed by another statistical technique called [Hierarchical Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering).\n",
    "\n",
    "üëâ Using [`scipy.cluster.hierarchy`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html), **plot the dendogram linkage** of the hierarchical clustering using the 'ward' method, that minimizes the within-cluster variance. You should get 4 clusters as well. \n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>üí° Solution</summary>\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "plt.figure(figsize=(20, 10))\n",
    "dendrogram(linkage(X, method='ward'))\n",
    "plt.show()\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit K-Means with 4 Optimal Clusters\n",
    "\n",
    "Now that we have found the optimal number of clusters, we can go on and fit a KMeans with 4 clusters over our observations. Scatter plot your observations, coloring every computed class with a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Note:** Does feature scaling always improve the clustering results? \n",
    "\n",
    "[Additional learning](https://datascience.stackexchange.com/questions/6715/is-it-necessary-to-standardize-your-data-before-clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Well done with your first challenge! **Don't forget to push your notebook.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
